{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPu93AfDy+YZ0Yk5dkXiIZT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UmarBalak/AdaptFL/blob/main/AdaptFL2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4TgIBGr0cQn"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import h5py\n",
        "import io\n",
        "\n",
        "# Load HDF5 file from a URL\n",
        "def load_hdf5_from_url(url, number_of_episodes=1):\n",
        "    hdf5_files = []\n",
        "    for episode_number in range(number_of_episodes):\n",
        "      new_url = url + f'episode_{episode_number}.hdf5'\n",
        "      response = requests.get(new_url)\n",
        "      response.raise_for_status()  # Raise an error for bad responses\n",
        "      hdf5_files.append(h5py.File(io.BytesIO(response.content), 'r'))\n",
        "      print(f\"episode_{episode_number}.hdf5 loaded successfully.\")\n",
        "    return hdf5_files\n",
        "\n",
        "hdf5_url = 'https://huggingface.co/datasets/nightmare-nectarine/segmentation-carla-driving/resolve/main/train/'\n",
        "hdf5_files = load_hdf5_from_url(hdf5_url, 6)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "\n",
        "# Explore the HDF5 file structure to see what datasets are present\n",
        "def explore_hdf5_structure(hdf5_file):\n",
        "    \"\"\"\n",
        "    Print the structure and contents of an HDF5 file to explore available datasets.\n",
        "    \"\"\"\n",
        "    def print_structure(name, obj):\n",
        "        if isinstance(obj, h5py.Group):\n",
        "            print(f\"Group: {name}\")\n",
        "        elif isinstance(obj, h5py.Dataset):\n",
        "            print(f\"Dataset: {name}, Shape: {obj.shape}, Type: {obj.dtype}\")\n",
        "\n",
        "    hdf5_file.visititems(print_structure)\n",
        "\n",
        "# Check each episode file's structure\n",
        "for episode_file in hdf5_files:\n",
        "    print(\"Exploring HDF5 structure:\")\n",
        "    explore_hdf5_structure(episode_file)\n"
      ],
      "metadata": {
        "id": "ABi7aedp0s6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "def preprocess_rgb_images(hdf5_files, image_size=(128, 128)):\n",
        "    \"\"\"\n",
        "    Preprocess multiple HDF5 files containing RGB images by normalizing and resizing them.\n",
        "\n",
        "    Parameters:\n",
        "    - hdf5_files: List of HDF5 files containing the 'rgb' dataset.\n",
        "    - image_size: Target size for resizing the images.\n",
        "\n",
        "    Returns:\n",
        "    - rgb_images_combined: Combined and preprocessed RGB images from all episodes.\n",
        "    \"\"\"\n",
        "    rgb_images_combined = []\n",
        "\n",
        "    # Loop through each HDF5 file and process the RGB images\n",
        "    for hdf5_file in hdf5_files:\n",
        "        rgb_images = hdf5_file['rgb'][:]\n",
        "\n",
        "        # Normalize RGB images\n",
        "        rgb_images = rgb_images.astype(np.float32) / 255.0\n",
        "\n",
        "        # Resize images\n",
        "        rgb_images_resized = np.array([cv2.resize(img, image_size) for img in rgb_images])\n",
        "\n",
        "        # Append to combined list\n",
        "        rgb_images_combined.append(rgb_images_resized)\n",
        "\n",
        "    # Concatenate all episodes into a single dataset\n",
        "    rgb_images_combined = np.concatenate(rgb_images_combined, axis=0)\n",
        "\n",
        "    return rgb_images_combined\n",
        "\n",
        "\n",
        "def preprocess_segmentation_masks(hdf5_files, image_size=(128, 128)):\n",
        "    \"\"\"\n",
        "    Preprocess multiple HDF5 files containing segmentation masks by resizing them.\n",
        "\n",
        "    Parameters:\n",
        "    - hdf5_files: List of HDF5 files containing the 'segmentation' dataset.\n",
        "    - image_size: Target size for resizing the masks.\n",
        "\n",
        "    Returns:\n",
        "    - segmentation_masks_combined: Combined and preprocessed segmentation masks from all episodes.\n",
        "    \"\"\"\n",
        "    segmentation_masks_combined = []\n",
        "\n",
        "    # Loop through each HDF5 file and process the segmentation masks\n",
        "    for hdf5_file in hdf5_files:\n",
        "        segmentation_masks = hdf5_file['segmentation'][:]\n",
        "\n",
        "        # Resize masks\n",
        "        segmentation_masks_resized = np.array([cv2.resize(mask, image_size) for mask in segmentation_masks])\n",
        "\n",
        "        # Append to combined list\n",
        "        segmentation_masks_combined.append(segmentation_masks_resized)\n",
        "\n",
        "    # Concatenate all episodes into a single dataset\n",
        "    segmentation_masks_combined = np.concatenate(segmentation_masks_combined, axis=0)\n",
        "\n",
        "    return segmentation_masks_combined\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def preprocess_controls_multiple_episodes(hdf5_files):\n",
        "    \"\"\"\n",
        "    Preprocess control data from multiple HDF5 files by concatenating and normalizing the controls.\n",
        "\n",
        "    Parameters:\n",
        "    - hdf5_files: List of HDF5 files containing the 'controls' dataset.\n",
        "\n",
        "    Returns:\n",
        "    - controls_normalized: Combined and normalized control data from all episodes.\n",
        "    \"\"\"\n",
        "    controls_combined = []\n",
        "\n",
        "    # Loop through each HDF5 file and concatenate the controls data\n",
        "    for hdf5_file in hdf5_files:\n",
        "        controls = hdf5_file['controls'][:]\n",
        "        controls_combined.append(controls)\n",
        "\n",
        "    # Concatenate all episodes' controls data\n",
        "    controls_combined = np.concatenate(controls_combined, axis=0)\n",
        "\n",
        "    # Normalize controls based on the max absolute value per feature\n",
        "    controls_normalized = controls_combined / np.max(np.abs(controls_combined), axis=0)\n",
        "\n",
        "    return controls_normalized\n",
        "\n",
        "def preprocess_frames_multiple_episodes(hdf5_files):\n",
        "    \"\"\"\n",
        "    Preprocess frame data from multiple HDF5 files by concatenating the frames.\n",
        "\n",
        "    Parameters:\n",
        "    - hdf5_files: List of HDF5 files containing the 'frame' dataset.\n",
        "\n",
        "    Returns:\n",
        "    - frames_combined: Combined frame data from all episodes.\n",
        "    \"\"\"\n",
        "    frames_combined = []\n",
        "\n",
        "    for hdf5_file in hdf5_files:\n",
        "        frames = hdf5_file['frame'][:]\n",
        "        frames_combined.append(frames)\n",
        "\n",
        "    frames_combined = np.concatenate(frames_combined, axis=0)\n",
        "    return frames_combined\n",
        "\n",
        "\n",
        "def preprocess_hlc_multiple_episodes(hdf5_files):\n",
        "    \"\"\"\n",
        "    Preprocess high-level command (hlc) data from multiple HDF5 files by concatenating the hlc data.\n",
        "\n",
        "    Parameters:\n",
        "    - hdf5_files: List of HDF5 files containing the 'hlc' dataset.\n",
        "\n",
        "    Returns:\n",
        "    - hlc_combined: Combined hlc data from all episodes.\n",
        "    \"\"\"\n",
        "    hlc_combined = []\n",
        "\n",
        "    for hdf5_file in hdf5_files:\n",
        "        hlc = hdf5_file['hlc'][:]\n",
        "        hlc_combined.append(hlc)\n",
        "\n",
        "    hlc_combined = np.concatenate(hlc_combined, axis=0)\n",
        "    return hlc_combined\n",
        "\n",
        "\n",
        "def preprocess_light_multiple_episodes(hdf5_files):\n",
        "    \"\"\"\n",
        "    Preprocess light data from multiple HDF5 files by concatenating the light data.\n",
        "\n",
        "    Parameters:\n",
        "    - hdf5_files: List of HDF5 files containing the 'light' dataset.\n",
        "\n",
        "    Returns:\n",
        "    - light_combined: Combined light data from all episodes.\n",
        "    \"\"\"\n",
        "    light_combined = []\n",
        "\n",
        "    for hdf5_file in hdf5_files:\n",
        "        light = hdf5_file['light'][:]\n",
        "        light_combined.append(light)\n",
        "\n",
        "    light_combined = np.concatenate(light_combined, axis=0)\n",
        "    return light_combined\n",
        "\n",
        "\n",
        "def preprocess_measurements_multiple_episodes(hdf5_files):\n",
        "    \"\"\"\n",
        "    Preprocess measurement data from multiple HDF5 files by concatenating the measurements.\n",
        "\n",
        "    Parameters:\n",
        "    - hdf5_files: List of HDF5 files containing the 'measurements' dataset.\n",
        "\n",
        "    Returns:\n",
        "    - measurements_combined: Combined measurement data from all episodes.\n",
        "    \"\"\"\n",
        "    measurements_combined = []\n",
        "\n",
        "    for hdf5_file in hdf5_files:\n",
        "        measurements = hdf5_file['measurements'][:]\n",
        "        measurements_combined.append(measurements)\n",
        "\n",
        "    measurements_combined = np.concatenate(measurements_combined, axis=0)\n",
        "    return measurements_combined\n",
        "\n",
        "\n",
        "# Call the functions\n",
        "rgb_data = preprocess_rgb_images(hdf5_files)\n",
        "segmentation_data = preprocess_segmentation_masks(hdf5_files)\n",
        "controls_normalized = preprocess_controls_multiple_episodes(hdf5_files)\n",
        "frame_data = preprocess_frames_multiple_episodes(hdf5_files)\n",
        "hlc_data = preprocess_hlc_multiple_episodes(hdf5_files)\n",
        "light_data = preprocess_light_multiple_episodes(hdf5_files)\n",
        "measurements_data = preprocess_measurements_multiple_episodes(hdf5_files)\n",
        "\n",
        "# Output shapes for verification\n",
        "print(f\"RGB images shape: {rgb_data.shape}\")\n",
        "print(f\"Segmentation masks shape: {segmentation_data.shape}\")\n",
        "print(f\"Controls shape: {controls_normalized.shape}\")\n",
        "print(f\"Frames shape: {frame_data.shape}\")\n",
        "print(f\"HLC shape: {hlc_data.shape}\")\n",
        "print(f\"Light shape: {light_data.shape}\")\n",
        "print(f\"Measurements shape: {measurements_data.shape}\")\n"
      ],
      "metadata": {
        "id": "SRaQhO041DKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_data_for_models_multiple_types(data_dict, num_models, data_distribution, data_types=None, random_seed=None):\n",
        "    \"\"\"\n",
        "    Split the preprocessed data into subsets for local models based on the specified distribution.\n",
        "\n",
        "    Parameters:\n",
        "    - data_dict: Dictionary containing various preprocessed data types (X, y, etc.).\n",
        "    - num_models: Number of local models to be implemented.\n",
        "    - data_distribution: List of proportions for each model's training data.\n",
        "    - data_types: List of data types to include in the split. If None, all data types are used.\n",
        "    - random_seed: Seed for reproducibility of data splits.\n",
        "\n",
        "    Returns:\n",
        "    - model_data: Dictionary containing training data for each model.\n",
        "    \"\"\"\n",
        "\n",
        "    # Validate inputs\n",
        "    if len(data_distribution) != num_models:\n",
        "        raise ValueError(\"Length of data_distribution must match num_models.\")\n",
        "    if not np.isclose(np.sum(data_distribution), 1.0):\n",
        "        raise ValueError(\"Data distribution must sum to 1.\")\n",
        "\n",
        "    # Set random seed for reproducibility\n",
        "    if random_seed is not None:\n",
        "        np.random.seed(random_seed)\n",
        "\n",
        "    model_data = {}\n",
        "    total_samples = len(data_dict['rgb'])  # Assumes all data types have the same length\n",
        "\n",
        "    # If data_types is None, use all keys from data_dict\n",
        "    if data_types is None:\n",
        "        data_types = data_dict.keys()\n",
        "\n",
        "    # Validate that all data types have the same number of samples\n",
        "    for key in data_types:\n",
        "        if key in data_dict and len(data_dict[key]) != total_samples:\n",
        "            raise ValueError(f\"Data type '{key}' has inconsistent sample size.\")\n",
        "\n",
        "    indices = np.arange(total_samples)\n",
        "    np.random.shuffle(indices)  # Shuffle indices for randomness\n",
        "\n",
        "    start_idx = 0\n",
        "    for model_id in range(num_models):\n",
        "        # Calculate the number of samples for the current model\n",
        "        num_samples = int(data_distribution[model_id] * total_samples)\n",
        "        if model_id == num_models - 1:  # Last model gets any leftover data\n",
        "            num_samples = total_samples - start_idx\n",
        "\n",
        "        model_data[f'model_{model_id + 1}'] = {\n",
        "            key: np.array([data_dict[key][i] for i in indices[start_idx:start_idx + num_samples]])\n",
        "            for key in data_types if key in data_dict\n",
        "        }\n",
        "\n",
        "        start_idx += num_samples\n",
        "\n",
        "    return model_data"
      ],
      "metadata": {
        "id": "xyeaEa-B1JJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dict = {\n",
        "    'controls': controls_normalized,\n",
        "    'frame': frame_data,\n",
        "    'hlc': hlc_data,\n",
        "    'light': light_data,\n",
        "    'measurements': measurements_data,\n",
        "    'rgb': rgb_data,\n",
        "    'segmentation': segmentation_data,\n",
        "}\n",
        "\n",
        "# Specify the number of local models, data distribution, and types to include\n",
        "num_local_models = 3\n",
        "data_distribution = [0.4, 0.4, 0.2]\n",
        "data_types_to_include = ['rgb', 'segmentation', 'hlc', 'light', 'measurements', 'controls', 'frame']\n",
        "\n",
        "# Split the data for selected types\n",
        "model_data = split_data_for_models_multiple_types(data_dict, num_local_models, data_distribution, data_types=data_types_to_include, random_seed=42)\n",
        "\n",
        "# Output shapes for verification\n",
        "for model_id, data in model_data.items():\n",
        "    print(f\"{model_id} - Data shapes:\")\n",
        "    for key, value in data.items():\n",
        "        print(f\"  {key}: {value.shape}\")"
      ],
      "metadata": {
        "id": "p7bNVoi31JMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model, Input\n",
        "\n",
        "# Defining the model architecture\n",
        "def create_model(input_shapes):\n",
        "    \"\"\"\n",
        "    Create a multi-input model based on the shapes of the input data.\n",
        "\n",
        "    Parameters:\n",
        "    - input_shapes: Dictionary containing the shape of each input type (e.g., 'rgb', 'segmentation', etc.)\n",
        "\n",
        "    Returns:\n",
        "    - A compiled Keras model with multiple inputs.\n",
        "    \"\"\"\n",
        "    # Input layers for each type of data\n",
        "    rgb_input = Input(shape=input_shapes['rgb'], name='rgb_input')\n",
        "    seg_input = Input(shape=input_shapes['segmentation'], name='seg_input')\n",
        "    hlc_input = Input(shape=input_shapes['hlc'], name='hlc_input')\n",
        "    light_input = Input(shape=input_shapes['light'], name='light_input')\n",
        "    measurements_input = Input(shape=input_shapes['measurements'], name='measurements_input')\n",
        "\n",
        "    # Simple CNN for RGB and Segmentation inputs\n",
        "    rgb_features = layers.Conv2D(32, (3, 3), activation='relu')(rgb_input)\n",
        "    rgb_features = layers.MaxPooling2D((2, 2))(rgb_features)\n",
        "    rgb_features = layers.Flatten()(rgb_features)\n",
        "\n",
        "    seg_features = layers.Conv2D(32, (3, 3), activation='relu')(seg_input)\n",
        "    seg_features = layers.MaxPooling2D((2, 2))(seg_features)\n",
        "    seg_features = layers.Flatten()(seg_features)\n",
        "\n",
        "    # Dense layers for other inputs\n",
        "    hlc_features = layers.Dense(16, activation='relu')(hlc_input)\n",
        "    light_features = layers.Dense(16, activation='relu')(light_input)\n",
        "    measurements_features = layers.Dense(16, activation='relu')(measurements_input)\n",
        "\n",
        "    # Concatenate all features\n",
        "    concatenated = layers.concatenate([rgb_features, seg_features, hlc_features, light_features, measurements_features])\n",
        "\n",
        "    # Output layer (adjust depending on your task, e.g., classification, regression)\n",
        "    output = layers.Dense(1, activation='sigmoid')(concatenated)\n",
        "\n",
        "    # Create model\n",
        "    model = Model(inputs=[rgb_input, seg_input, hlc_input, light_input, measurements_input], outputs=output)\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "CP61MRG11JPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "local_models = []\n",
        "for model_id, data in model_data.items():\n",
        "    print(f\"Training {model_id}...\")\n",
        "\n",
        "    # Get the shapes of the inputs\n",
        "    input_shapes = {key: data[key].shape[1:] for key in data_types_to_include}\n",
        "\n",
        "    # Create a model for this data\n",
        "    model = create_model(input_shapes)\n",
        "\n",
        "    # Train the model (placeholder example using dummy labels)\n",
        "    labels = data['controls']  # Shape should be (num_samples, 3)\n",
        "\n",
        "    model.fit(\n",
        "        [data['rgb'], data['segmentation'], data['hlc'], data['light'], data['measurements']],\n",
        "        labels, epochs=10, batch_size=16\n",
        "    )\n",
        "\n",
        "    local_models.append(model)\n",
        "    print(f\"{model_id} training complete.\\n\")"
      ],
      "metadata": {
        "id": "p3LOHl7h1JRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iXAenJq11JT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iMnQWvsR1JaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b8PrjbO31JdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xaf_RFg31Jfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FSNtG4pU1JiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-yxWf3-z1Jlk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}